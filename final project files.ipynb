{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82ae1c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##importing our libraries\n",
    "import pandas as pd\n",
    "import docx2pdf\n",
    "from docx2pdf import convert\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk import ne_chunk,pos_tag,word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import pandas \n",
    "import phonenumbers\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords=set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6fe54c",
   "metadata": {},
   "source": [
    "# # extracting personal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1311cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give me the path of your resume:C:\\Users\\asus\\Documents\\mon_cv.pdf\n",
      "PERSONAL INFORAMTIONS:\n",
      "name:  Ihssane Nedjaoui \n",
      "email address: ihssanenedjaoui5 @ gmail.com\n",
      "phone_number: 2020-2021\n",
      "adress: 105 Rue Fes Qu Jerifate Safi \n",
      "EDUCATION:\n",
      "EXPERIENCE: ['engineering student ']\n"
     ]
    }
   ],
   "source": [
    "###    OUR CODE START HERE\n",
    "\n",
    "\n",
    "#parsing our pdf\n",
    "path=str(input('give me the path of your resume:'))\n",
    "if path[-3:]!='pdf':\n",
    "    convert(path,r'C:\\Users\\asus\\Documents\\new_file.pdf')\n",
    "    path=r'C:\\Users\\asus\\Documents\\new_file.pdf'\n",
    "def parsing_pdf(path):\n",
    "    file =open(path,'rb')\n",
    "    pdf_reader=PyPDF2.PdfFileReader(file)\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    global clear_text\n",
    "    clear_text=''\n",
    "    global text\n",
    "    text=\"\"\n",
    "    num_of_pages=pdf_reader.numPages\n",
    "    for i in range(num_of_pages):\n",
    "        text+= pdf_reader.getPage(i).extractText()+\"  \" # return a text format of the pdf resume\n",
    "    text=text.replace('\\n', ' ')\n",
    "    filtered_words=[word for word in word_tokenize(text) if word not in stopwords]\n",
    "    clear_text=''\n",
    "    for word in filtered_words:\n",
    "            clear_text=clear_text+lemmatizer.lemmatize(word)+' '\n",
    "    text=clear_text\n",
    "    \n",
    "                \n",
    "    \n",
    "parsing_pdf(path)\n",
    "\n",
    "        \n",
    "        \n",
    "def extracting_names(text):\n",
    "    chunks=ne_chunk(pos_tag(word_tokenize(text))) \n",
    "    global names\n",
    "    names=[]                                                         \n",
    "    for chunk in chunks:\n",
    "        if type(chunk)==Tree and chunk.label()=='PERSON':\n",
    "                for leaf in chunk.leaves():\n",
    "                    names.append(leaf[0])               #return a list that contain potential name of person  \n",
    "                    \n",
    "                    \n",
    "def extracting_email(text):\n",
    "    reg_exps=['\\S+ @ gmail.com']\n",
    "    for reg_exp in reg_exps:  \n",
    "        global email\n",
    "        email =re.findall(reg_exp,text)                 \n",
    "extracting_email(text)\n",
    "extracting_names(text)\n",
    "\n",
    "\n",
    "#we will try to find the exacte name of the person  \n",
    "user_name=email[0][:email[0].index('@')]\n",
    "real_name=\" \"\n",
    "for name in names:\n",
    "    if name.lower() in user_name:\n",
    "        real_name+=name + \" \"\n",
    "        \n",
    "        \n",
    "    \n",
    "def extracting_adress(text):\n",
    "    data=pd.read_csv(r\"C:\\Users\\asus\\Downloads\\ma.csv\")\n",
    "    global df_data\n",
    "    df_data=pd.DataFrame(data)\n",
    "    for citi in df_data['city']:\n",
    "        if citi in text:\n",
    "            our_city=citi\n",
    "    numbers=[]\n",
    "    for word in pos_tag(word_tokenize(text)):\n",
    "        if word[1]=='CD' and our_city in text[text.index(word[0]):]:\n",
    "            numbers.append(word[0])\n",
    "    global adress\n",
    "    adress=\"\"\n",
    "    for item in numbers:\n",
    "        our_substring=text[text.index(item):text.index(item)+30]\n",
    "        if our_city in our_substring:          \n",
    "            adress+=our_substring[our_substring.index(item):our_substring.index(our_city)+1+len(our_city)]+\"\"\n",
    "            break \n",
    "        else:\n",
    "            print('extraction of address is failed') #it return address as a string \n",
    "            break\n",
    "def extracting_phonenumber(text):\n",
    "    phone_numbers=phonenumbers.PhoneNumberMatcher(text,'MR')\n",
    "    for number in phone_numbers:\n",
    "        global phone_number\n",
    "        phone_number=str(number)[29:len(str(number))+1] \n",
    "        break\n",
    "\n",
    "        \n",
    "def extracting_education(text):\n",
    "    chunks=ne_chunk(pos_tag(word_tokenize(text))) \n",
    "    reserved_names=['School','University','College','Baccalaureat','Master','Licence','Doctorat']\n",
    "    global names\n",
    "    names=[]\n",
    "    global institution\n",
    "    institution=[]\n",
    "    for chunk in chunks:\n",
    "        if type(chunk)==Tree and chunk.label()=='ORGANIZATION':\n",
    "                for leaf in chunk.leaves():\n",
    "                    names.append(leaf[0])    ##it include the names of organization that can be a name institute\n",
    "    for name in names:\n",
    "        if name in reserved_names :\n",
    "            institution.append(text[text.index(name):text.index(name)+30])  \n",
    "            \n",
    "            \n",
    "            \n",
    "def extracting_experience(text):\n",
    "    job_title=['Team_Leader','Manager','Assistant_Manager','Executive','Director','Developer''Coordinator','Administrator',\n",
    "               'Controller','Officer','Organizer','Supervisor','Superintendent','engineering',\n",
    "               'Head','Overseer','Chief','Foreman','Controller','Principal','President','Lead','Intern']\n",
    "    global experiences\n",
    "    experiences=[]\n",
    "    for job in job_title:\n",
    "        if job in text:\n",
    "            experiences.append(text[text.index(job):text.index(job)+20])\n",
    "    \n",
    "extracting_experience(text)           \n",
    "                        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "print('PERSONAL INFORAMTIONS:')\n",
    "extracting_phonenumber(text)       \n",
    "extracting_adress(text)    \n",
    "print('name:',real_name)\n",
    "print('email address:',email[0])\n",
    "print('phone_number:',phone_number)\n",
    "print('adress:',adress)\n",
    "extracting_education(text)\n",
    "print('EDUCATION:')\n",
    "institution\n",
    "extracting_experience(text)\n",
    "print('EXPERIENCE:',experiences)\n",
    "\n",
    "\n",
    "\n",
    "#C:\\Users\\asus\\Downloads\\CV_2022-06-13_IHSSANE_NEDJAOUI.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "634fe47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['School Of Applied Science Safi']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "institution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be2757b",
   "metadata": {},
   "source": [
    "# # education "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce86fc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['School Of Applied Science Safi']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extracting_education(text):\n",
    "    chunks=ne_chunk(pos_tag(word_tokenize(text))) \n",
    "    reserved_names=['School','University','College','Baccalaureat','Master','Licence']\n",
    "    global names\n",
    "    names=[]\n",
    "    global institution\n",
    "    institution=[]\n",
    "    for chunk in chunks:\n",
    "        if type(chunk)==Tree and chunk.label()=='ORGANIZATION':\n",
    "                for leaf in chunk.leaves():\n",
    "                    names.append(leaf[0])    ##it include the names of organization that can be a name institute\n",
    "    for name in names:\n",
    "        if name in reserved_names :\n",
    "            institution.append(text[text.index(name):text.index(name)+30])\n",
    "    return institution       \n",
    "extracting_education(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "edf45229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extracting_experience(text):\n",
    "    global job_title\n",
    "    job_title=['Team_Leader','Manager','Assistant_Manager','Executive','Director','Developer''Coordinator','Administrator',\n",
    "               'Controller','Officer','Organizer','Supervisor','Superintendent',\n",
    "               'Head','Overseer','Chief','Foreman','Controller','Principal','President','Lead','Intern','Ingeneer']\n",
    "    experiences=[]\n",
    "    for job in job_title:\n",
    "        if job in text:\n",
    "            expriences.append(text[text.index(job)-10:text.index(job)+10])\n",
    "    return experiences\n",
    "extracting_experience(text)           \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
